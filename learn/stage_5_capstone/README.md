# üéì Stage 5: Job-Ready Capstone

**Build Your Testing Portfolio**

You've learned unit, integration, E2E, performance, and security testing. Now it's time to showcase your skills! Build a complete test suite, document your work, and create portfolio artifacts that prove you're job-ready.

## Your Progress

[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100% complete
‚úÖ Stage 1: Unit Tests (completed)
‚úÖ Stage 2: Integration Tests (completed)
‚úÖ Stage 3: API & E2E Testing (completed)
‚úÖ Stage 4: Performance & Security (completed)
‚Üí **Stage 5: Capstone** (you are here)

**Estimated time remaining:** 2-3 hours

---

## üéØ Learning Goals

By the end of this stage, you will:

- ‚úÖ Build a complete test suite from scratch
- ‚úÖ Create professional testing documentation
- ‚úÖ Capture visual artifacts (screenshots, videos)
- ‚úÖ Write resume-worthy project descriptions
- ‚úÖ Prepare for QA engineering interviews
- ‚úÖ Understand how to present your testing work
- ‚úÖ Know what employers look for in QA candidates

**Duration:** 2-3 hours (Quick Path) | 6-10 hours (Comprehensive Path)

**üìö Essential Resource: [Portfolio Guide](../../docs/guides/PORTFOLIO.md)** - Complete guide on turning your work into resume content, interview prep, and job-ready portfolio projects!

---

## üìã Capstone Project

### Project: Build a Test Suite for a New Feature

Choose one of these options:

#### Option A: Messaging Feature

Imagine Testbook adds direct messaging. Build tests for:

- Sending messages
- Receiving messages
- Message history
- Read receipts
- Message notifications

#### Option B: Bookmarks Feature

Imagine users can bookmark posts. Build tests for:

- Adding bookmarks
- Viewing bookmarks
- Removing bookmarks
- Bookmark count updates
- Bookmark permissions

#### Option C: Search Feature

Imagine Testbook adds search. Build tests for:

- Searching posts
- Searching users
- Search filters
- Empty results
- Search performance

---

## üèóÔ∏è Building Your Suite

### Step 1: Plan Your Tests

Before writing code, plan your test strategy:

**Create a test plan document:**

```markdown
# Test Plan: [Feature Name]

## Feature Overview

[Describe what the feature does]

## Test Strategy

### Unit Tests (Fast, isolated)

- [ ] Test 1: ...
- [ ] Test 2: ...

### Integration Tests (API endpoints)

- [ ] Test 1: ...
- [ ] Test 2: ...

### E2E Tests (User workflows)

- [ ] Test 1: ...
- [ ] Test 2: ...

### Security Tests

- [ ] Test 1: ...
- [ ] Test 2: ...

## Test Data Requirements

- User accounts: ...
- Sample data: ...

## Success Criteria

- All tests pass
- Coverage > 80%
- All endpoints tested
- Security verified
```

### Step 2: Write Unit Tests

Create `test_unit_<feature>.py`:

```python
import pytest
from your_feature import some_function

@pytest.mark.unit
class TestFeatureLogic:
    """Unit tests for [feature] business logic."""

    def test_basic_functionality(self):
        """Verify basic function works correctly."""
        # Arrange
        input_data = "test"

        # Act
        result = some_function(input_data)

        # Assert
        assert result is not None

    def test_edge_case(self):
        """Verify handling of edge cases."""
        # Test with empty input, None, etc.
        pass
```

### Step 3: Write Integration Tests

Create `test_api_<feature>.py`:

```python
import pytest

@pytest.mark.integration
@pytest.mark.api
class TestFeatureAPI:
    """Integration tests for [feature] API endpoints."""

    def test_create_resource(self, client, auth_headers):
        """Test creating a new resource."""
        # Arrange
        payload = {"data": "test"}

        # Act
        response = client.post("/feature", json=payload, headers=auth_headers)

        # Assert
        assert response.status_code == 201
        assert "id" in response.json()

    def test_unauthorized_access(self, client):
        """Verify authentication is required."""
        response = client.post("/feature", json={})
        assert response.status_code == 401
```

### Step 4: Write E2E Tests

Create `test_e2e_<feature>.py`:

```python
import pytest
from playwright.async_api import expect

@pytest.mark.e2e
async def test_user_can_use_feature(page, login):
    """End-to-end test of complete user workflow."""
    # Navigate to feature
    await page.goto("http://localhost:3000/feature")

    # Interact with UI
    await page.fill("#input", "test data")
    await page.click("#submit")

    # Verify result
    await expect(page.locator(".success-message")).to_be_visible()
```

### Step 5: Add Security Tests

```python
@pytest.mark.security
def test_feature_security(client, auth_headers):
    """Verify security measures in feature."""
    # Test SQL injection
    malicious_input = "'; DROP TABLE--"
    response = client.post("/feature",
        json={"data": malicious_input},
        headers=auth_headers
    )
    assert response.status_code in [400, 422]
```

### Step 6: Run and Document Results

```bash
# Run all tests with coverage
pytest --cov=your_feature --cov-report=html

# Run E2E tests with video
VIDEO_ON_FAILURE=true HEADLESS=false pytest test_e2e_<feature>.py

# Generate report
pytest --html=report.html
```

---

## üì∏ Creating Portfolio Artifacts

### Artifact 1: Test Coverage Report

```bash
# Generate HTML coverage report
pytest --cov --cov-report=html

# Screenshot the coverage report
# Open htmlcov/index.html and capture screenshot
```

**Include in portfolio:**

- Overall coverage percentage
- File-by-file coverage
- Highlighted: "Achieved 85% test coverage"

### Artifact 2: Test Execution Video

```bash
# Run E2E test with recording
VIDEO_ON_FAILURE=true HEADLESS=false pytest tests/test_e2e_feature.py

# Result: video of test running
```

**Include in portfolio:**

- 30-60 second clip
- Shows test in action
- Voiceover explaining what's being tested (optional)

### Artifact 3: Test Report

Generate professional test report:

```bash
pytest --html=test-report.html --self-contained-html
```

**Include in portfolio:**

- Pass/fail statistics
- Test execution time
- Any failures and how you fixed them

### Artifact 4: Test Documentation

Create `TESTING.md` for your feature:

<!-- markdownlint-disable MD040 -->

````markdown
# Testing Documentation: [Feature Name]

## Test Coverage

- **Unit Tests:** 15 tests, 100% function coverage
- **Integration Tests:** 12 tests, all endpoints covered
- **E2E Tests:** 5 tests, critical paths verified
- **Security Tests:** 3 tests, OWASP Top 10 addressed

## Running Tests

```bash
# Run all tests
pytest -v

# Run specific suite
pytest -m unit
pytest -m integration
pytest -m e2e
```

## Test Strategy

### Unit Tests

Focus on business logic validation...

### Integration Tests

Verify API contracts and data flow...

### E2E Tests

Confirm user workflows work end-to-end...

## Notable Test Cases

### Test: Unauthorized Access Prevention

**Why it matters:** Prevents users from accessing others' data
**How it works:** Attempts to access resource with wrong credentials
**Result:** Returns 403 Forbidden

## Continuous Improvement

Areas for future testing:

- Performance testing with k6
- Accessibility testing
- Cross-browser E2E tests
````

<!-- markdownlint-enable MD040 -->

---

## üìù Resume & Portfolio

### Resume Bullet Points

Use these templates (fill in your specifics):

**For Testbook:**

- Built comprehensive test automation suite with pytest and Playwright, achieving 84% code coverage across 100+ tests
- Implemented E2E testing framework using Page Object Model pattern, reducing test maintenance time by 40%
- Developed security test suite covering OWASP Top 10 vulnerabilities, identifying and documenting 3 critical issues
- Created performance testing strategy using k6, establishing baseline metrics for 500 concurrent users

**For Your Capstone:**

- Designed and implemented test automation for [feature] including unit, integration, E2E, and security tests
- Achieved 85%+ test coverage using pytest, FastAPI TestClient, and Playwright
- Documented testing strategy and created reusable test fixtures and factories
- Produced portfolio artifacts including coverage reports, test videos, and comprehensive documentation

### Portfolio README Template

Create `PORTFOLIO.md`:

```markdown
# Testing Portfolio - [Your Name]

## Overview

I completed Testbook's 5-stage learning path and built a comprehensive test suite demonstrating professional QA engineering skills.

## Skills Demonstrated

### Test Automation

- Pytest (unit and integration testing)
- Playwright (E2E browser automation)
- k6 (performance testing)
- Test fixture design
- Page Object Model pattern

### Testing Types

- ‚úÖ Unit testing
- ‚úÖ Integration testing
- ‚úÖ API testing
- ‚úÖ E2E testing
- ‚úÖ Security testing
- ‚úÖ Performance testing

### Tools & Technologies

- Python, pytest, Playwright
- FastAPI, PostgreSQL
- Git, GitHub Actions (CI/CD)
- k6, coverage.py

## Projects

### 1. Testbook Learning Path

**Duration:** [Your timeframe]
**Description:** Completed 5-stage curriculum covering all aspects of test automation

**Achievements:**

- Wrote 50+ tests across all test types
- Achieved 85%+ coverage
- Implemented Page Object Model
- Created security test suite

[Link to code] | [Video demo] | [Coverage report]

### 2. Capstone: [Feature] Test Suite

**Description:** Built complete test automation for [feature] from scratch

**Test Coverage:**

- 15 unit tests
- 12 integration tests
- 5 E2E tests
- 3 security tests

**Results:**

- 87% code coverage
- All critical paths tested
- 0 production bugs in 30 days

[Link to code] | [Documentation] | [Demo video]

## Metrics

| Project  | Tests | Coverage | Status      |
| -------- | ----- | -------- | ----------- |
| Testbook | 100+  | 84%      | ‚úÖ Complete |
| Capstone | 35    | 87%      | ‚úÖ Complete |

## Contact

[Your professional links]
```

---

## üé§ Interview Preparation

### Common QA Interview Questions

Practice answering these with examples from your work:

#### Technical Questions

**Q: "Explain the difference between unit, integration, and E2E tests."**
_Use the test pyramid. Give examples from Testbook._

**Q: "How do you decide what to test?"**
_Talk about risk-based testing, critical paths, test strategy._

**Q: "Walk me through how you'd test [feature]."**
_Show your test plan from this capstone._

**Q: "How do you handle flaky tests?"**
_Discuss waits, retries, isolation, debugging techniques._

**Q: "What's your approach to security testing?"**
_Reference OWASP Top 10, your security tests._

#### Behavioral Questions

**Q: "Tell me about a bug you found."**
_Prepare story: What was it? How'd you find it? Impact? How'd you document it?_

**Q: "Describe a time you had to learn a new testing tool."**
_Your Testbook journey! Playwright, pytest, k6._

**Q: "How do you prioritize testing when time is limited?"**
_Risk-based testing, smoke tests, critical paths._

### Show Your Work

**Bring to interviews:**

1. **Laptop with code ready** - Demo your tests running
2. **Coverage report screenshot** - Visual proof
3. **Test execution video** - Show E2E tests in action
4. **TESTING.md document** - Professional documentation
5. **GitHub repo link** - Clean, public portfolio

**Practice this demo:**
"Here's a test I wrote for [feature]. Let me show you how it runs..."
[Run test, explain each step, show passing result]

---

## ‚úÖ Capstone Checklist

You're portfolio-ready when you have:

### Tests Written

- [ ] 10+ unit tests with good coverage
- [ ] 8+ integration tests covering all endpoints
- [ ] 3+ E2E tests for critical workflows
- [ ] 2+ security tests for OWASP risks
- [ ] All tests passing

### Documentation Created

- [ ] Test plan document
- [ ] TESTING.md with instructions
- [ ] Code comments explaining complex tests
- [ ] README with clear setup instructions
- [ ] **üìö [Portfolio guide](../../docs/guides/PORTFOLIO.md) followed** for resume/LinkedIn

### Artifacts Captured

- [ ] Coverage report (HTML screenshot)
- [ ] Test execution video (1-2 minutes)
- [ ] Test report (pytest HTML report)
- [ ] Before/after metrics (if applicable)

### Portfolio Prepared

- [ ] GitHub repo is public and clean
- [ ] README explains project clearly
- [ ] Code is well-organized and formatted
- [ ] No sensitive data or credentials
- [ ] Professional commit history

### Interview Ready

- [ ] Can demo tests running live
- [ ] Can explain testing strategy
- [ ] Can discuss challenges and solutions
- [ ] Have prepared answers to common questions
- [ ] Resume includes testing experience

---

## üéØ Next Steps After Completion

### 1. Share Your Work

**Post on LinkedIn:**

```text
Excited to share that I completed Testbook's comprehensive test automation curriculum!

üß™ Built test suites covering unit, integration, E2E, performance, and security testing
üõ†Ô∏è Technologies: Python, pytest, Playwright, k6, FastAPI
üìä Achieved 85%+ test coverage across 50+ tests
üéØ Created portfolio-ready artifacts and documentation

Check out my work: [GitHub link]

#QA #TestAutomation #Python #Playwright #Testing
```

### 2. Contribute to Testbook

Now that you understand the codebase:

- Add new tests
- Improve documentation
- Fix bugs
- Help other learners

### 3. Apply Your Skills

**Job Search:**

- Update resume with Testbook experience
- Apply for junior QA/SDET roles
- Use portfolio in applications
- Practice technical interviews

**Personal Projects:**

- Add testing to your own projects
- Test open source projects
- Build testing tools
- Write testing blog posts

### 4. Keep Learning

**Advanced Topics:**

- CI/CD pipelines (GitHub Actions)
- Test automation frameworks
- Mobile testing (Appium)
- API testing (Postman, REST Assured)
- Performance testing (JMeter)

---

## üèÜ Congratulations

You've completed all 5 stages of the Testbook learning path!

### What You've Accomplished

‚úÖ **Mastered test automation** from unit to E2E
‚úÖ **Built professional test suites** with 85%+ coverage
‚úÖ **Learned industry-standard tools** (pytest, Playwright, k6)
‚úÖ **Created portfolio artifacts** for job applications
‚úÖ **Developed QA engineering mindset** (quality, security, user experience)

### You're Now Ready For

üéØ **Junior QA Engineer** roles
üéØ **SDET (Software Development Engineer in Test)** roles
üéØ **Test Automation Engineer** roles
üéØ **Quality Assurance** positions

---

## üöÄ CI/CD & Automation

Professional testing teams use continuous integration and deployment pipelines to automatically run tests on every code change. This section teaches you how to set up production-ready CI/CD for both Python and JavaScript testing stacks.

### üéØ Learning Objectives

By the end of this section, you will be able to:

- ‚úÖ Configure GitHub Actions for automated testing
- ‚úÖ Set up caching strategies for faster CI runs
- ‚úÖ Capture and manage test artifacts (screenshots, videos, reports)
- ‚úÖ Implement retry strategies for flaky tests
- ‚úÖ Run tests in parallel for efficiency
- ‚úÖ Upload coverage reports and track metrics
- ‚úÖ Handle test failures gracefully with notifications

---

## üìã Prerequisites

Before starting this section, ensure you have:

- ‚úÖ Tests working locally (Python and/or JavaScript)
- ‚úÖ GitHub repository (or GitLab/other CI platform)
- ‚úÖ Basic understanding of YAML configuration
- ‚úÖ Completed [Stage 3: API & E2E Testing](../stage_3_api_e2e/README.md)

---

## üèóÔ∏è Architecture Overview

### CI/CD Pipeline Structure

```text
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    GitHub Actions CI/CD                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Push/PR ‚Üí Install ‚Üí Cache ‚Üí Test ‚Üí Report ‚Üí Deploy       ‚îÇ
‚îÇ     ‚Üì         ‚Üì        ‚Üì       ‚Üì       ‚Üì        ‚Üì          ‚îÇ
‚îÇ  Trigger  Dependencies Browser  Run   Upload   Notify      ‚îÇ
‚îÇ           (pip/npm)   Install  Tests Results  Team        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Test Execution Strategy

```text
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Test Execution Flow                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Backend Tests ‚Üí Frontend Tests ‚Üí E2E Tests ‚Üí Security     ‚îÇ
‚îÇ     (Fast)        (Medium)       (Slow)      (Optional)    ‚îÇ
‚îÇ     ‚Üì              ‚Üì              ‚Üì             ‚Üì          ‚îÇ
‚îÇ  Fail Fast    Parallel Run   Full Browser   OWASP Scan    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìö Section 1: Understanding CI/CD for Testing

### What is CI/CD?

**Continuous Integration (CI):**

- Automatically runs tests on every code change
- Catches bugs early before they reach production
- Provides immediate feedback to developers

**Continuous Deployment (CD):**

- Automatically deploys code that passes all tests
- Reduces manual deployment errors
- Enables rapid, reliable releases

### Why E2E Tests in CI/CD?

**Benefits:**

- ‚úÖ Catch integration issues between frontend and backend
- ‚úÖ Validate complete user workflows
- ‚úÖ Test across multiple browsers and environments
- ‚úÖ Ensure production readiness

**Challenges:**

- ‚ö†Ô∏è Slower execution than unit tests
- ‚ö†Ô∏è More complex setup and maintenance
- ‚ö†Ô∏è Can be flaky due to timing issues
- ‚ö†Ô∏è Requires running application stack

---

## üõ†Ô∏è Section 2: GitHub Actions Setup

### Basic Workflow Structure

Every GitHub Actions workflow follows this pattern:

```yaml
name: Workflow Name
on: [trigger events]
jobs:
  job-name:
    runs-on: [operating system]
    steps:
      - name: Step Name
        run: commands
```

### Essential Actions for Testing

**Core Actions:**

- `actions/checkout@v4` - Get your code
- `actions/setup-python@v4` - Python environment
- `actions/setup-node@v4` - Node.js environment
- `actions/upload-artifact@v3` - Save test results

---

## üêç Section 3: Python E2E Testing Pipeline

### Basic Python E2E Workflow

Create `.github/workflows/e2e-python.yml`:

```yaml
name: E2E Tests (Python/Playwright)

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]

jobs:
  e2e-python:
    name: Python E2E Tests
    runs-on: ubuntu-latest

    strategy:
      fail-fast: false
      matrix:
        browser: [chromium, firefox]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.13"
          cache: "pip"

      - name: Install Python dependencies
        run: |
          pip install -r tests/e2e-python/requirements.txt

      - name: Install Playwright browsers
        run: |
          playwright install --with-deps chromium  # Chrome only for faster CI

      - name: Start backend
        run: |
          cd backend
          source .venv/bin/activate || python -m venv .venv && source .venv/bin/activate
          pip install -r requirements.txt
          uvicorn main:app --host 0.0.0.0 --port 8000 &
          sleep 5

      - name: Start frontend
        run: |
          cd frontend
          npm ci
          npm run dev &
          sleep 10
          curl http://localhost:3000 || (echo "Frontend failed to start" && exit 1)

      - name: Run E2E tests
        env:
          BROWSER: ${{ matrix.browser }}
          HEADLESS: true
          BASE_URL: http://localhost:3000
          API_URL: http://localhost:8000
        run: |
          cd tests/e2e-python
          HEADLESS=true pytest --browser=${{ matrix.browser }} -v --tb=short

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: e2e-python-${{ matrix.browser }}-results
          path: |
            tests/e2e-python/test-results/
            tests/e2e-python/screenshots/
          retention-days: 30

      - name: Upload videos on failure
        if: failure()
        uses: actions/upload-artifact@v3
        with:
          name: e2e-python-${{ matrix.browser }}-videos
          path: tests/e2e-python/test-results/videos/
          retention-days: 30
```

### Key Features Explained

**Matrix Strategy:**

- Tests multiple browsers in parallel
- `fail-fast: false` ensures all browsers are tested even if one fails

**Service Startup:**

- Backend and frontend started as background processes
- Health checks ensure services are ready before tests

**Artifact Management:**

- Test results saved for debugging
- Videos only uploaded on failure to save space

---

## üü® Section 4: JavaScript E2E Testing Pipeline

### Basic JavaScript E2E Workflow

Create `.github/workflows/e2e-javascript.yml`:

```yaml
name: E2E Tests (JavaScript/Playwright)

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]

jobs:
  e2e-javascript:
    name: JavaScript E2E Tests
    runs-on: ubuntu-latest

    strategy:
      fail-fast: false
      matrix:
        browser: [chromium, firefox, webkit]
        shard: [1, 2] # Run in 2 parallel shards

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "18"
          cache: "npm"

      - name: Install dependencies
        run: |
          cd tests
          npm ci

      - name: Install Playwright browsers
        run: |
          cd tests
          npx playwright install --with-deps chromium  # Chrome only for faster CI

      - name: Start backend
        run: |
          cd backend
          python -m venv .venv
          source .venv/bin/activate
          pip install -r requirements.txt
          uvicorn main:app --host 0.0.0.0 --port 8000 &
          sleep 5

      - name: Start frontend
        run: |
          cd frontend
          npm ci
          npm run dev &
          sleep 10

      - name: Run E2E tests
        run: |
          cd tests
          npx playwright test --project=${{ matrix.browser }} --shard=${{ matrix.shard }}/${{ strategy.job-total }}

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: e2e-js-${{ matrix.browser }}-shard-${{ matrix.shard }}-results
          path: |
            tests/playwright-report/
            tests/test-results/
          retention-days: 30

      - name: Upload Playwright report
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: playwright-report-${{ matrix.browser }}-${{ matrix.shard }}
          path: tests/playwright-report/
          retention-days: 30
```

### Advanced Features

**Test Sharding:**

- Splits tests across multiple parallel jobs
- Reduces total execution time
- `strategy.job-total` automatically calculates total shards

**Multi-Browser Testing:**

- Tests across Chromium, Firefox, and WebKit
- Ensures cross-browser compatibility
- Each browser runs in separate job

---

## üß© Section 5: Component Testing Integration

### Vitest Component Test Workflow

Create `.github/workflows/component-tests.yml`:

```yaml
name: Component Tests (Vitest)

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]

jobs:
  component-tests:
    name: React Component Tests
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "18"
          cache: "npm"
          cache-dependency-path: frontend/package-lock.json

      - name: Install dependencies
        run: |
          cd frontend
          npm ci

      - name: Run component tests
        run: |
          cd frontend
          npm test -- --coverage

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          files: ./frontend/coverage/coverage-final.json
          flags: component-tests
          name: component-coverage

      - name: Upload coverage report
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: component-test-coverage
          path: frontend/coverage/
          retention-days: 30
```

### Coverage Integration

**Codecov Integration:**

- Automatic coverage reporting
- Trend tracking over time
- Coverage badges for README

**Local Coverage Reports:**

- HTML reports for detailed analysis
- Artifacts saved for 30 days
- Easy access to coverage data

---

## üîß Section 6: Backend Testing Pipeline

### Backend Test Workflow

Create `.github/workflows/backend-tests.yml`:

```yaml
name: Backend Tests (pytest)

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]

jobs:
  backend-tests:
    name: Backend API Tests
    runs-on: ubuntu-latest

    strategy:
      matrix:
        python-version: ["3.11", "3.12", "3.13"]

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: testuser
          POSTGRES_PASSWORD: testpass
          POSTGRES_DB: testdb
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
          cache: "pip"

      - name: Install dependencies
        run: |
          cd backend
          pip install -r requirements.txt
          pip install pytest-cov

      - name: Run tests with coverage
        env:
          DATABASE_URL: postgresql://testuser:testpass@localhost:5432/testdb
        run: |
          cd backend
          pytest --cov --cov-report=xml --cov-report=html -v

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          files: ./backend/coverage.xml
          flags: backend-tests
          name: backend-coverage-py${{ matrix.python-version }}

      - name: Upload coverage report
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: backend-coverage-py${{ matrix.python-version }}
          path: backend/htmlcov/
          retention-days: 30
```

### Multi-Version Testing

**Python Version Matrix:**

- Tests across multiple Python versions
- Ensures compatibility with different environments
- Catches version-specific issues

**Database Services:**

- Uses GitHub Actions services for PostgreSQL
- Health checks ensure database is ready
- Isolated test environment

---

## üöÄ Section 7: Complete Test Suite Workflow

### All-in-One Workflow

Create `.github/workflows/test-suite.yml`:

```yaml
name: Complete Test Suite

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  backend-tests:
    name: Backend Tests
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v4
        with:
          python-version: "3.13"
          cache: "pip"
      - run: |
          cd backend
          pip install -r requirements.txt pytest-cov
          pytest --cov --cov-report=xml
      - uses: codecov/codecov-action@v3
        with:
          flags: backend

  component-tests:
    name: Component Tests
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: "18"
          cache: "npm"
          cache-dependency-path: frontend/package-lock.json
      - run: |
          cd frontend
          npm ci
          npm test -- --coverage
      - uses: codecov/codecov-action@v3
        with:
          flags: component

  e2e-tests:
    name: E2E Tests
    runs-on: ubuntu-latest
    needs: [backend-tests, component-tests] # Run after unit tests pass

    strategy:
      matrix:
        test-suite: [python, javascript]

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.13"

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "18"

      - name: Install Playwright
        run: |
          if [ "${{ matrix.test-suite }}" == "python" ]; then
            pip install -r tests/e2e-python/requirements.txt
            playwright install --with-deps chromium  # Chrome only for faster CI
          else
            cd tests && npm ci
            npx playwright install --with-deps chromium  # Chrome only for faster CI
          fi

      - name: Start services
        run: |
          # Start backend
          cd backend
          pip install -r requirements.txt
          uvicorn main:app --port 8000 &

          # Start frontend
          cd ../frontend
          npm ci
          npm run dev &

          # Wait for services
          sleep 15

      - name: Run E2E tests
        run: |
          if [ "${{ matrix.test-suite }}" == "python" ]; then
            cd tests/e2e-python
            pytest -v
          else
            cd tests
            npx playwright test
          fi

      - uses: actions/upload-artifact@v3
        if: always()
        with:
          name: e2e-${{ matrix.test-suite }}-results
          path: tests/**/test-results/

  security-tests:
    name: Security Tests
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v4
        with:
          python-version: "3.13"
      - run: |
          cd backend
          pip install -r requirements.txt
          uvicorn main:app --port 8000 &
          sleep 5
          cd ../tests/security
          pip install -r requirements.txt
          pytest -v
```

### Job Dependencies

**Execution Order:**

1. Backend tests (fastest)
2. Component tests (medium speed)
3. E2E tests (slowest, only if unit tests pass)
4. Security tests (optional, can run in parallel)

**Benefits:**

- Fast feedback on unit tests
- E2E tests only run if unit tests pass
- Parallel execution where possible
- Comprehensive coverage

---

## ‚ö° Section 8: Optimization Strategies

### Caching Dependencies

**Python Caching:**

```yaml
- uses: actions/setup-python@v4
  with:
    python-version: "3.13"
    cache: "pip"
    cache-dependency-path: "**/requirements.txt"
```

**Node.js Caching:**

```yaml
- uses: actions/setup-node@v4
  with:
    node-version: "18"
    cache: "npm"
    cache-dependency-path: "**/package-lock.json"
```

### Caching Playwright Browsers

```yaml
- name: Cache Playwright browsers
  uses: actions/cache@v3
  id: playwright-cache
  with:
    path: ~/.cache/ms-playwright
    key: ${{ runner.os }}-playwright-${{ hashFiles('**/package-lock.json') }}

- name: Install Playwright browsers
  if: steps.playwright-cache.outputs.cache-hit != 'true'
  run: npx playwright install --with-deps chromium # Chrome only for faster CI
```

### Parallel Test Execution

**Playwright Sharding:**

```yaml
strategy:
  matrix:
    shard: [1, 2, 3, 4]

steps:
  - run: npx playwright test --shard=${{ matrix.shard }}/4
```

**pytest-xdist:**

```yaml
- run: pytest -n auto # Auto-detect CPU count
```

---

## üîÑ Section 9: Retry Strategies

### Playwright Retry Configuration

**playwright.config.js:**

```javascript
module.exports = {
  retries: process.env.CI ? 2 : 0, // Retry 2x in CI
  workers: process.env.CI ? 2 : undefined,
};
```

### pytest Retry Configuration

**pytest.ini:**

```ini
[pytest]
addopts = --reruns 2 --reruns-delay 1
```

Install: `pip install pytest-rerun-failures`

---

## üö® Section 10: Handling Test Failures

### Slack Notifications

```yaml
- name: Notify Slack on failure
  if: failure()
  uses: slackapi/slack-github-action@v1
  with:
    payload: |
      {
        "text": "E2E Tests Failed!",
        "blocks": [
          {
            "type": "section",
            "text": {
              "type": "mrkdwn",
              "text": "E2E tests failed on ${{ github.ref }}"
            }
          }
        ]
      }
  env:
    SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }}
```

### GitHub Pull Request Comments

```yaml
- name: Comment PR with test results
  if: github.event_name == 'pull_request'
  uses: actions/github-script@v6
  with:
    script: |
      github.rest.issues.createComment({
        issue_number: context.issue.number,
        owner: context.repo.owner,
        repo: context.repo.repo,
        body: '‚úÖ All E2E tests passed!'
      })
```

---

## üê≥ Section 11: Docker-Based CI

### Using Docker Compose in CI

```yaml
jobs:
  e2e-docker:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Start services with Docker Compose
        run: docker-compose up -d

      - name: Wait for services
        run: |
          timeout 60 bash -c 'until curl -f http://localhost:3000; do sleep 2; done'
          timeout 60 bash -c 'until curl -f http://localhost:8000/docs; do sleep 2; done'

      - name: Run E2E tests
        run: |
          docker-compose exec -T backend pytest tests/
          docker-compose exec -T frontend npm test

      - name: Cleanup
        if: always()
        run: docker-compose down -v
```

---

## üìä Section 12: Monitoring & Metrics

### Key Metrics to Track

1. **Test execution time** - Should stay under 10 minutes
2. **Flake rate** - Should be < 1%
3. **Coverage percentage** - Track trends
4. **Failure rate** - Monitor for patterns
5. **CI queue time** - Optimize if growing

### Tracking with Codecov

```yaml
- uses: codecov/codecov-action@v3
  with:
    files: ./coverage/coverage-final.json
    flags: e2e-tests
    fail_ci_if_error: true
```

---

## üéì Best Practices Summary

### ‚úÖ Do's

- **Cache dependencies** - Speeds up CI significantly
- **Run unit tests first** - Fail fast strategy
- **Retry flaky tests** - 1-2 retries in CI only
- **Capture artifacts** - Screenshots, videos, reports
- **Upload coverage** - Track coverage trends
- **Use matrix builds** - Test across browsers/versions
- **Parallelize tests** - Reduce CI time
- **Use specific browser versions** - Avoid surprises

### ‚ùå Don'ts

- **Don't skip tests in CI** - CI should be stricter than local
- **Don't ignore flaky tests** - Fix the root cause
- **Don't run all tests serially** - Use parallel execution
- **Don't use `latest` tags** - Pin versions for consistency
- **Don't store secrets in code** - Use CI secrets
- **Don't run E2E if unit tests fail** - Save CI time

---

## üöÄ Implementation Steps

### Step 1: Start Simple

1. Create basic workflow for your primary test suite
2. Add caching for dependencies
3. Upload test artifacts

### Step 2: Add Optimization

1. Implement test sharding
2. Add retry strategies
3. Optimize execution order

### Step 3: Add Monitoring

1. Integrate coverage reporting
2. Add failure notifications
3. Track key metrics

### Step 4: Scale Up

1. Add multi-browser testing
2. Implement parallel execution
3. Add security and performance tests

---

## üìö Related Resources

- [Testing Guide](../../docs/guides/TESTING_GUIDE.md) - Comprehensive testing examples
- [Stage 3: API & E2E Testing](../stage_3_api_e2e/README.md) - E2E testing fundamentals
- [Troubleshooting Guide](../../docs/reference/TROUBLESHOOTING.md) - Fix common CI/CD issues
- [GitHub Actions Documentation](https://docs.github.com/en/actions)
- [Playwright CI Guide](https://playwright.dev/docs/ci)

---

**üéâ You now have professional CI/CD pipelines for automated testing! This is exactly what production teams use to ensure code quality and reliability.**

---

## üìû Stay Connected

**Completed Testbook?**

- ‚≠ê Star the repo on GitHub
- üí¨ Share your experience in Discussions
- üêõ Report bugs you find
- üìù Contribute improvements
- ü§ù Help other learners

---

## üß† Final Self-Check Quiz (Optional)

Before completing Testbook, can you answer these questions?

1. **What's the main purpose of a CI/CD pipeline?**

   - A) To make tests run faster
   - B) To automatically test code when changes are made
   - C) To organize test files
   - D) To create test data

2. **What should you include in a testing portfolio?**

   - A) Only passing tests
   - B) Test results, code samples, and documentation
   - C) Only test failures
   - D) Only unit tests

3. **Why is documentation important in testing?**

   - A) It makes tests run faster
   - B) It helps others understand and maintain tests
   - C) It's required by law
   - D) It's not important

4. **What's the difference between a test plan and test cases?**

   - A) There's no difference
   - B) Test plan is high-level strategy, test cases are specific steps
   - C) Test cases are faster
   - D) Test plan is for unit tests only

5. **What makes a good QA engineer?**
   - A) Only technical skills
   - B) Only communication skills
   - C) Technical skills, communication, and problem-solving
   - D) Only experience

**Answers:** [Check your answers here](solutions/stage_5_quiz_answers.md)

---

## üéâ Final Reflection

Take a moment to look back at your journey:

1. **What was the most challenging part?**
2. **What surprised you most about testing?**
3. **What's your favorite type of testing and why?**
4. **How has your understanding of software quality changed?**
5. **What's next in your QA journey?**

**Document your answers** in [reflection.md](reflection.md).

---

## üöÄ You Did It

You're no longer learning test automation ‚Äî you're practicing it professionally.

**Go build amazing things. Test them well. Ship them confidently.**

---

_Remember: Every expert was once a beginner. You've done the hard work. Now go show the world what you can do! üí™_

---

**Need help or have questions?**

- Check [README.md](../../README.md#frequently-asked-questions) - Learning questions and quick setup guidance
- Read [CONTRIBUTING](../../CONTRIBUTING.md)
- Open a GitHub Discussion
- Connect with the community

**Thank you for completing Testbook! üéì‚ú®**
